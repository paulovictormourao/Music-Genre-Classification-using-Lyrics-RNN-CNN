{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T02:55:47.665408Z",
     "iopub.status.busy": "2024-06-03T02:55:47.664862Z",
     "iopub.status.idle": "2024-06-03T02:56:05.125017Z",
     "shell.execute_reply": "2024-06-03T02:56:05.123918Z",
     "shell.execute_reply.started": "2024-06-03T02:55:47.665358Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 15:17:57.267695: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict,train_test_split, StratifiedKFold \n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Concatenate, Bidirectional, Embedding\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization, GRU, Masking\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import he_normal, glorot_normal, glorot_uniform\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Nadam, Adagrad, Adamax, RMSprop\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tabs_new_line(lyrics):\n",
    "    string = re.sub(r\"\\s+\",\" \",lyrics)\n",
    "    string = re.sub(r\"[/]\",\" \",string)\n",
    "    return string\n",
    "\n",
    "def decontractions(lyrics):\n",
    "    decontracted = re.sub(r\"won\\'t\", \"will not\", lyrics)\n",
    "    decontracted = re.sub(r\"can\\'t\", \"can not\", decontracted)\n",
    "    decontracted = re.sub(r\"n\\'t\", \" not\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'re\", \" are\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'s\", \" is\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'d\", \" would\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'ll\", \" will\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'t\", \" not\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'ve\", \" have\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'m\", \" am\", decontracted)\n",
    "    decontracted = re.sub(r\"ain\\'t\", \"is not\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'cause\", \"because\", decontracted)\n",
    "    decontracted = re.sub(r\"y\\'all\", \"you all\", decontracted)\n",
    "    decontracted = re.sub(r\"ma\\'am\", \"madam\", decontracted)\n",
    "    decontracted = re.sub(r\"o\\'clock\", \"of the clock\", decontracted)\n",
    "    decontracted = re.sub(r\"gonna\", \"going to\", decontracted)\n",
    "    decontracted = re.sub(r\"wanna\", \"want to\", decontracted)\n",
    "    decontracted = re.sub(r\"gotta\", \"got to\", decontracted)\n",
    "    decontracted = re.sub(r\"let\\'s\", \"let us\", decontracted)\n",
    "    decontracted = re.sub(r\"how\\'d\", \"how did\", decontracted)\n",
    "    decontracted = re.sub(r\"how\\'ll\", \"how will\", decontracted)\n",
    "    decontracted = re.sub(r\"how\\'s\", \"how is\", decontracted)\n",
    "    decontracted = re.sub(r\"what\\'d\", \"what did\", decontracted)\n",
    "    decontracted = re.sub(r\"what\\'ll\", \"what will\", decontracted)\n",
    "    decontracted = re.sub(r\"what\\'re\", \"what are\", decontracted)\n",
    "    decontracted = re.sub(r\"what\\'s\", \"what is\", decontracted)\n",
    "    decontracted = re.sub(r\"what\\'ve\", \"what have\", decontracted)\n",
    "    decontracted = re.sub(r\"where\\'d\", \"where did\", decontracted)\n",
    "    decontracted = re.sub(r\"where\\'ll\", \"where will\", decontracted)\n",
    "    decontracted = re.sub(r\"where\\'re\", \"where are\", decontracted)\n",
    "    decontracted = re.sub(r\"where\\'s\", \"where is\", decontracted)\n",
    "    decontracted = re.sub(r\"where\\'ve\", \"where have\", decontracted)\n",
    "    decontracted = re.sub(r\"who\\'d\", \"who did\", decontracted)\n",
    "    decontracted = re.sub(r\"who\\'ll\", \"who will\", decontracted)\n",
    "    decontracted = re.sub(r\"who\\'re\", \"who are\", decontracted)\n",
    "    decontracted = re.sub(r\"who\\'s\", \"who is\", decontracted)\n",
    "    decontracted = re.sub(r\"who\\'ve\", \"who have\", decontracted)\n",
    "    decontracted = re.sub(r\"why\\'d\", \"why did\", decontracted)\n",
    "    decontracted = re.sub(r\"why\\'ll\", \"why will\", decontracted)\n",
    "    decontracted = re.sub(r\"why\\'re\", \"why are\", decontracted)\n",
    "    decontracted = re.sub(r\"why\\'s\", \"why is\", decontracted)\n",
    "    decontracted = re.sub(r\"why\\'ve\", \"why have\", decontracted)\n",
    "    decontracted = re.sub(r\"that\\'d\", \"that would\", decontracted)\n",
    "    decontracted = re.sub(r\"that\\'ll\", \"that will\", decontracted)\n",
    "    decontracted = re.sub(r\"that\\'re\", \"that are\", decontracted)\n",
    "    decontracted = re.sub(r\"that\\'s\", \"that is\", decontracted)\n",
    "    decontracted = re.sub(r\"that\\'ve\", \"that have\", decontracted)\n",
    "    return decontracted\n",
    "\n",
    "def lower_case(lyrics):\n",
    "    return lyrics.lower() \n",
    "\n",
    "def remove_stopwords(tokens):  \n",
    "    tokens_without_stopwords = [word for word in tokens if word not in STOP_WORDS]\n",
    "    return tokens_without_stopwords\n",
    "\n",
    "def remove_short_words(tokens, N):\n",
    "    filtered_lyrics = [word for word in tokens if len(word) > N]  # Filtra palavras com tamanho >= N\n",
    "    return filtered_lyrics\n",
    "\n",
    "def remove_repeated_words(tokens):\n",
    "    unique_words = []\n",
    "    seen_words = set()\n",
    "    for word in tokens:\n",
    "        if word not in seen_words:\n",
    "            unique_words.append(word)\n",
    "            seen_words.add(word)\n",
    "    return unique_words\n",
    "\n",
    "def final_processing(lyrics):  \n",
    "    string = re.sub(r'[^a-z\\s]', '', lyrics)\n",
    "    string = re.sub(r'\\bverse\\b', '', string)\n",
    "    string = re.sub(r'\\bchorus\\b', '', string)\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(lyrics):\n",
    "    lyrics = remove_tabs_new_line(lyrics)\n",
    "    lyrics = lower_case(lyrics)\n",
    "    lyrics = decontractions(lyrics) # Decontractions like: I've -> I have\n",
    "    tokens = word_tokenize(lyrics) \n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = remove_short_words(tokens, N=2)\n",
    "    #tokens = remove_repeated_words(tokens)\n",
    "    lyrics = ' '.join(tokens)\n",
    "    lyrics = final_processing(lyrics)\n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T02:56:05.127651Z",
     "iopub.status.busy": "2024-06-03T02:56:05.126996Z",
     "iopub.status.idle": "2024-06-03T02:56:05.133502Z",
     "shell.execute_reply": "2024-06-03T02:56:05.132328Z",
     "shell.execute_reply.started": "2024-06-03T02:56:05.127618Z"
    }
   },
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "DATA_PATH = '/home/paulo/anaconda3/envs/studies/TMCI_Project-master/lyrics.csv' \n",
    "df_full = pd.read_csv(DATA_PATH)\n",
    "df_full = df_full.dropna()\n",
    "df_full = df_full.drop(df_full.loc[(df_full['year'] < 1900) | (df_full['year'] > 2024)].index)\n",
    "df_full = df_full.drop(df_full[(df_full['genre'] == 'Not Available') | (df_full['genre'] == 'Other')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full[['lyrics', 'genre']]\n",
    "\n",
    "def clean_lyrics(text):\n",
    "    text = re.sub(r'[^A-Za-z ]', ' ', text) # Remover caracteres especiais\n",
    "    \n",
    "    words = text.split()  # Remover letras com menos de 3 palavras\n",
    "    if len(words) < 3:\n",
    "        return \"\"\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text.strip()  \n",
    " \n",
    "    \n",
    "df_full['clean_lyrics'] = df_full['lyrics'].apply(clean_lyrics)\n",
    "\n",
    "df_clean = df_full[df_full['clean_lyrics'] != \"\"] # Excluir linhas com menos de 3 palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(df, target_column, max_instances):\n",
    "    balanced_df = pd.DataFrame()\n",
    "    for class_label, group in df.groupby(target_column):\n",
    "        if len(group) > max_instances:\n",
    "            # Ordena o grupo pelo n√∫mero de palavras e seleciona os primeiros max_instances\n",
    "            group = group.assign(word_count=group[target_column].apply(lambda x: len(str(x).split())))\n",
    "            group = group.sort_values(by='word_count', ascending=False).head(max_instances)\n",
    "        balanced_df = pd.concat([balanced_df, group])\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_instances_per_class = 2095\n",
    "balanced_df = balance_classes(df_clean, 'genre',max_instances_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df['processed_lyrics'] = balanced_df['lyrics'].apply(preprocess_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_frequency_treshhold(word_count, N):\n",
    "    N_most_common_words = []\n",
    "    for word, count in word_count.items():\n",
    "        if count > N:\n",
    "            N_most_common_words.append(word)\n",
    "    return N_most_common_words\n",
    "\n",
    "def remove_words(text, words_to_remove):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word not in words_to_remove]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frequency = 1600\n",
    "embeddings_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics = ' '.join(balanced_df['processed_lyrics'])\n",
    "words = all_lyrics.split()\n",
    "\n",
    "word_count = Counter(words)\n",
    "\n",
    "most_common_words = words_frequency_treshhold(word_count, max_frequency)\n",
    "\n",
    "balanced_df['processed_lyrics'] = balanced_df['processed_lyrics'].apply(lambda lyrics: remove_words(lyrics, most_common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "\n",
    "with open('vectors'+str(max_frequency)+'.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=np.float32)\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(balanced_df['processed_lyrics'])\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, embeddings_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df['encoded_docs'] = tokenizer.texts_to_sequences(balanced_df['processed_lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "Hip-Hop       2052\n",
       "Folk          2020\n",
       "Metal         2020\n",
       "Pop           2018\n",
       "Indie         1988\n",
       "Country       1986\n",
       "R&B           1981\n",
       "Rock          1965\n",
       "Jazz          1942\n",
       "Electronic    1820\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtred = balanced_df[balanced_df['encoded_docs'].apply(len) >= 14]\n",
    "df_filtred['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshhold = min(df_filtred['genre'].value_counts().tolist())\n",
    "true_balanced_df = balance_classes(df_filtred, 'genre', 1801)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 500\n",
    "X = pad_sequences(true_balanced_df['encoded_docs'], maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(true_balanced_df['genre'])\n",
    "y = one_hot.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim(y) = (18010, 10) Dim(X) = (18010, 500)\n"
     ]
    }
   ],
   "source": [
    "print(f'Dim(y) = {y.shape} Dim(X) = {X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_layer = Input(shape=(max_length,), dtype='int32')\n",
    "    \n",
    "    masked_input = Masking(mask_value=0)(input_layer)\n",
    "\n",
    "    e = Embedding(vocab_size, embeddings_dim, weights=[embedding_matrix], input_length=max_length, trainable=True)(masked_input)\n",
    "\n",
    "    lstm_l = Bidirectional(LSTM(32, return_sequences=False, kernel_initializer=glorot_uniform(),recurrent_dropout=0.2))(e)\n",
    "    lstm_l = Dropout(0.1)(lstm_l)\n",
    "    lstm_l = BatchNormalization()(lstm_l)\n",
    "\n",
    "    dense_layer = Dense(64, activation=\"relu\",kernel_initializer=he_normal(),kernel_regularizer=l2(0.001))(lstm_l)\n",
    "    dense_layer = Dropout(0.1)(dense_layer)\n",
    "    dense_layer = BatchNormalization()(dense_layer)\n",
    "    \n",
    "\n",
    "    output_layer = Dense(10, activation=\"softmax\", kernel_initializer=glorot_normal())(dense_layer)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T02:56:08.250669Z",
     "iopub.status.busy": "2024-06-03T02:56:08.250326Z",
     "iopub.status.idle": "2024-06-03T02:58:48.183672Z",
     "shell.execute_reply": "2024-06-03T02:58:48.182556Z",
     "shell.execute_reply.started": "2024-06-03T02:56:08.250641Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch 1/32\n",
      "451/451 [==============================] - 4245s 9s/step - loss: 2.2019 - accuracy: 0.2889 - val_loss: 1.9428 - val_accuracy: 0.3584\n",
      "Epoch 2/32\n",
      "451/451 [==============================] - 2517s 6s/step - loss: 1.5972 - accuracy: 0.4874 - val_loss: 1.8613 - val_accuracy: 0.3970\n",
      "Epoch 3/32\n",
      "451/451 [==============================] - 2473s 5s/step - loss: 0.9844 - accuracy: 0.7050 - val_loss: 2.0522 - val_accuracy: 0.4067\n",
      "Epoch 4/32\n",
      "138/451 [========>.....................] - ETA: 29:22 - loss: 0.5187 - accuracy: 0.8662"
     ]
    }
   ],
   "source": [
    "num_folds = 5\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "f1_per_fold = []\n",
    "loss_per_fold = []\n",
    "conf_matrix_list = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X, np.argmax(y, axis=1))):\n",
    "    \n",
    "    print(f'Fold {fold_idx + 1}/{num_folds}')\n",
    "    \n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    model = create_model()\n",
    "    early_stopping = EarlyStopping(patience=4, restore_best_weights=True)\n",
    "    history = model.fit(X_train,y_train, batch_size=32, epochs=32, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "    y_pred = model.predict(X_val)  \n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(np.argmax(y_val.astype(int), axis=1),y_pred_classes)\n",
    "    conf_matrix_list.append(conf_matrix)\n",
    "    \n",
    "    f1 = f1_score(np.argmax(y_val, axis=1), y_pred_classes, average='weighted') \n",
    "    f1_per_fold.append(f1)\n",
    "    \n",
    "    scores = model.evaluate(X_val, y_val) \n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    print(f'F1 Score do fold {fold_idx + 1}: {f1}')\n",
    "\n",
    "print(f'\\nF1 Score m√©dio nos {num_folds} folds: {np.mean(f1_per_fold)} (+/- {np.std(f1_per_fold)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T02:58:48.186397Z",
     "iopub.status.busy": "2024-06-03T02:58:48.185875Z",
     "iopub.status.idle": "2024-06-03T02:58:48.948576Z",
     "shell.execute_reply": "2024-06-03T02:58:48.947435Z",
     "shell.execute_reply.started": "2024-06-03T02:58:48.186363Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs_range = range(1, len(history.epoch) + 1)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Train Set')\n",
    "plt.plot(epochs_range, val_acc, label='Val Set')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Train Set')\n",
    "plt.plot(epochs_range, val_loss, label='Val Set')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "conf_matrix = np.sum(conf_matrix_list, axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(balanced_df['genre']), yticklabels=np.unique(balanced_df['genre']))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 165566,
     "sourceId": 377107,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30715,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "studies",
   "language": "python",
   "name": "studies"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
