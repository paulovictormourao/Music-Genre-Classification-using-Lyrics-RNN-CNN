{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T02:55:47.665408Z",
     "iopub.status.busy": "2024-06-03T02:55:47.664862Z",
     "iopub.status.idle": "2024-06-03T02:56:05.125017Z",
     "shell.execute_reply": "2024-06-03T02:56:05.123918Z",
     "shell.execute_reply.started": "2024-06-03T02:55:47.665358Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 15:24:57.073457: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict,train_test_split, StratifiedKFold \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Concatenate, concatenate, Embedding\n",
    "from tensorflow.keras.layers import MaxPooling1D, Conv1D, GlobalMaxPooling1D, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import he_normal, glorot_normal\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Nadam, Adagrad, Adamax\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T02:56:05.127651Z",
     "iopub.status.busy": "2024-06-03T02:56:05.126996Z",
     "iopub.status.idle": "2024-06-03T02:56:05.133502Z",
     "shell.execute_reply": "2024-06-03T02:56:05.132328Z",
     "shell.execute_reply.started": "2024-06-03T02:56:05.127618Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/paulo/anaconda3/envs/studies/TMCI_Project-master/lyrics.csv' \n",
    "df_full = pd.read_csv(DATA_PATH)\n",
    "df_full = df_full.dropna()\n",
    "df_full = df_full.drop(df_full.loc[(df_full['year'] < 1900) | (df_full['year'] > 2024)].index)\n",
    "df_full = df_full.drop(df_full[(df_full['genre'] == 'Not Available') | (df_full['genre'] == 'Other')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full[['lyrics', 'genre']]\n",
    "\n",
    "def clean_lyrics(text):\n",
    "    text = re.sub(r'[^A-Za-z ]', ' ', text) # Remover caracteres especiais\n",
    "    \n",
    "    words = text.split()  # Remover letras com menos de 3 palavras\n",
    "    if len(words) < 3:\n",
    "        return \"\"\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text.strip()  \n",
    " \n",
    "    \n",
    "df_full['clean_lyrics'] = df_full['lyrics'].apply(clean_lyrics)\n",
    "\n",
    "df_clean = df_full[df_full['clean_lyrics'] != \"\"] # Excluir linhas com menos de 3 palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T02:56:05.160592Z",
     "iopub.status.busy": "2024-06-03T02:56:05.160101Z",
     "iopub.status.idle": "2024-06-03T02:56:07.740956Z",
     "shell.execute_reply": "2024-06-03T02:56:07.739521Z",
     "shell.execute_reply.started": "2024-06-03T02:56:05.160550Z"
    }
   },
   "outputs": [],
   "source": [
    "genre_frequency = df_clean['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(df, target_column, max_instances):\n",
    "    balanced_df = pd.DataFrame() # Ordena o grupo pelo número de palavras e seleciona os primeiros max_instances\n",
    "    for class_label, group in df.groupby(target_column):\n",
    "        if len(group) > max_instances:\n",
    "            group = group.assign(word_count=group[target_column].apply(lambda x: len(str(x).split())))\n",
    "            group = group.sort_values(by='word_count', ascending=False).head(max_instances)\n",
    "        balanced_df = pd.concat([balanced_df, group])\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_instances_per_class = 3390\n",
    "balanced_df = balance_classes(df_clean, 'genre',max_instances_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tabs_new_line(lyrics):\n",
    "    string = re.sub(r\"\\s+\",\" \",lyrics)\n",
    "    string = re.sub(r\"[/]\",\" \",string)\n",
    "    return string\n",
    "\n",
    "def decontractions(lyrics):\n",
    "    decontracted = re.sub(r\"won\\'t\", \"will not\", lyrics)\n",
    "    decontracted = re.sub(r\"can\\'t\", \"can not\", decontracted)\n",
    "    decontracted = re.sub(r\"n\\'t\", \" not\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'re\", \" are\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'s\", \" is\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'d\", \" would\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'ll\", \" will\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'t\", \" not\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'ve\", \" have\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'m\", \" am\", decontracted)\n",
    "    decontracted = re.sub(r\"ain\\'t\", \"is not\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'cause\", \"because\", decontracted)\n",
    "    decontracted = re.sub(r\"y\\'all\", \"you all\", decontracted)\n",
    "    decontracted = re.sub(r\"ma\\'am\", \"madam\", decontracted)\n",
    "    decontracted = re.sub(r\"o\\'clock\", \"of the clock\", decontracted)\n",
    "    decontracted = re.sub(r\"gonna\", \"going to\", decontracted)\n",
    "    decontracted = re.sub(r\"wanna\", \"want to\", decontracted)\n",
    "    decontracted = re.sub(r\"gotta\", \"got to\", decontracted)\n",
    "    decontracted = re.sub(r\"let\\'s\", \"let us\", decontracted)\n",
    "    decontracted = re.sub(r\"how\\'d\", \"how did\", decontracted)\n",
    "    decontracted = re.sub(r\"how\\'ll\", \"how will\", decontracted)\n",
    "    decontracted = re.sub(r\"how\\'s\", \"how is\", decontracted)\n",
    "    decontracted = re.sub(r\"what\\'d\", \"what did\", decontracted)\n",
    "    decontracted = re.sub(r\"what\\'ll\", \"what will\", decontracted)\n",
    "    decontracted = re.sub(r\"what\\'re\", \"what are\", decontracted)\n",
    "    decontracted = re.sub(r\"what\\'s\", \"what is\", decontracted)\n",
    "    decontracted = re.sub(r\"what\\'ve\", \"what have\", decontracted)\n",
    "    decontracted = re.sub(r\"where\\'d\", \"where did\", decontracted)\n",
    "    decontracted = re.sub(r\"where\\'ll\", \"where will\", decontracted)\n",
    "    decontracted = re.sub(r\"where\\'re\", \"where are\", decontracted)\n",
    "    decontracted = re.sub(r\"where\\'s\", \"where is\", decontracted)\n",
    "    decontracted = re.sub(r\"where\\'ve\", \"where have\", decontracted)\n",
    "    decontracted = re.sub(r\"who\\'d\", \"who did\", decontracted)\n",
    "    decontracted = re.sub(r\"who\\'ll\", \"who will\", decontracted)\n",
    "    decontracted = re.sub(r\"who\\'re\", \"who are\", decontracted)\n",
    "    decontracted = re.sub(r\"who\\'s\", \"who is\", decontracted)\n",
    "    decontracted = re.sub(r\"who\\'ve\", \"who have\", decontracted)\n",
    "    decontracted = re.sub(r\"why\\'d\", \"why did\", decontracted)\n",
    "    decontracted = re.sub(r\"why\\'ll\", \"why will\", decontracted)\n",
    "    decontracted = re.sub(r\"why\\'re\", \"why are\", decontracted)\n",
    "    decontracted = re.sub(r\"why\\'s\", \"why is\", decontracted)\n",
    "    decontracted = re.sub(r\"why\\'ve\", \"why have\", decontracted)\n",
    "    decontracted = re.sub(r\"that\\'d\", \"that would\", decontracted)\n",
    "    decontracted = re.sub(r\"that\\'ll\", \"that will\", decontracted)\n",
    "    decontracted = re.sub(r\"that\\'re\", \"that are\", decontracted)\n",
    "    decontracted = re.sub(r\"that\\'s\", \"that is\", decontracted)\n",
    "    decontracted = re.sub(r\"that\\'ve\", \"that have\", decontracted)\n",
    "    return decontracted\n",
    "\n",
    "def lower_case(lyrics):\n",
    "    return lyrics.lower() \n",
    "\n",
    "def is_english(lyrics):\n",
    "    try:\n",
    "        detect(lyrics) == 'en'\n",
    "        return lyrics\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "def remove_stopwords(tokens):  \n",
    "    tokens_without_stopwords = [word for word in tokens if word not in STOP_WORDS]\n",
    "    return tokens_without_stopwords\n",
    "\n",
    "def remove_short_words(tokens, N):\n",
    "    filtered_lyrics = [word for word in tokens if len(word) > N]  # Filtra palavras com tamanho >= N\n",
    "    return filtered_lyrics\n",
    "\n",
    "def remove_repeated_words(tokens):\n",
    "    unique_words = []\n",
    "    seen_words = set()\n",
    "    for word in tokens:\n",
    "        if word not in seen_words:\n",
    "            unique_words.append(word)\n",
    "            seen_words.add(word)\n",
    "    return unique_words\n",
    "\n",
    "def final_processing(lyrics):  \n",
    "    string = re.sub(r'[^a-z\\s]', '', lyrics)\n",
    "    string = re.sub(r'\\bverse\\b', '', string)\n",
    "    string = re.sub(r'\\bchorus\\b', '', string)\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(lyrics):\n",
    "    lyrics = remove_tabs_new_line(lyrics)\n",
    "    lyrics = lower_case(lyrics)\n",
    "    #lyrics = is_english(lyrics)\n",
    "    lyrics = decontractions(lyrics) # Decontractions like: I've -> I have\n",
    "    tokens = word_tokenize(lyrics) \n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = remove_short_words(tokens, N=2)\n",
    "    #tokens = remove_repeated_words(tokens)\n",
    "    lyrics = ' '.join(tokens)\n",
    "    lyrics = final_processing(lyrics)\n",
    "    \n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df['processed_lyrics'] = balanced_df['lyrics'].apply(preprocess_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_frequency_treshhold(word_count, N):\n",
    "    N_most_common_words = []\n",
    "    for word, count in word_count.items():\n",
    "        if count > N:\n",
    "            N_most_common_words.append(word)\n",
    "    return N_most_common_words\n",
    "\n",
    "def remove_words(text, words_to_remove):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word not in words_to_remove]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frequency = 1900\n",
    "embeddings_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics = ' '.join(balanced_df['processed_lyrics'])\n",
    "words = all_lyrics.split()\n",
    "\n",
    "word_count = Counter(words)\n",
    "\n",
    "most_common_words = words_frequency_treshhold(word_count, max_frequency)\n",
    "\n",
    "\n",
    "balanced_df['processed_lyrics'] = balanced_df['processed_lyrics'].apply(lambda lyrics: remove_words(lyrics, most_common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "\n",
    "with open('unbalanced_vectors'+str(max_frequency)+'.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=np.float32)\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(balanced_df['processed_lyrics'])\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, embeddings_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = tokenizer.texts_to_sequences(balanced_df['processed_lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32316,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df['encoded_docs'] = encoded_docs\n",
    "balanced_df['encoded_docs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "Hip-Hop       3269\n",
       "Metal         2988\n",
       "Pop           2957\n",
       "Country       2761\n",
       "Rock          2753\n",
       "R&B           2651\n",
       "Jazz          2548\n",
       "Indie         2537\n",
       "Electronic    2525\n",
       "Folk          1880\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtred = balanced_df[balanced_df['encoded_docs'].apply(len) >= 20]\n",
    "df_filtred['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 170\n",
    "X = pad_sequences(df_filtred['encoded_docs'], maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26869, 170)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(df_filtred['genre'])\n",
    "y = one_hot.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_layer =  Input(shape=(max_length),dtype='int32')\n",
    "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=True)(input_layer)\n",
    "    \n",
    "    x1=Conv1D(3,7,kernel_initializer='glorot_uniform',kernel_regularizer=l2(),activation='relu')(e)\n",
    "    x2=Conv1D(3,8,kernel_initializer='glorot_uniform',kernel_regularizer=l2(),activation='relu')(e)\n",
    "    x3=Conv1D(3,6,kernel_initializer='glorot_uniform',kernel_regularizer=l2(),activation='relu')(e)\n",
    "\n",
    "    concatted1= concatenate([x1,x2,x3],axis=1)\n",
    "    \n",
    "    max_pool1=MaxPooling1D(3)(concatted1)\n",
    "\n",
    "    y1=Conv1D(3,9,kernel_initializer='glorot_uniform',activation='relu')(max_pool1)\n",
    "    y2=Conv1D(3,4,kernel_initializer='glorot_uniform',activation='relu')(max_pool1)\n",
    "    y3=Conv1D(3,5,kernel_initializer='glorot_uniform',activation='relu')(max_pool1)\n",
    "\n",
    "    concatted2 =concatenate([y1,y2,y3],axis=1)\n",
    "    max_pool2=MaxPooling1D(3)(concatted2)\n",
    "    \n",
    "    drop_out=Dropout(0.5)(max_pool1)\n",
    "    max_pool3=MaxPooling1D(3)(drop_out)\n",
    "\n",
    "    conv_layer1=Conv1D(3,12, activation='relu')(max_pool3)\n",
    "\n",
    "    flatten=Flatten()(conv_layer1)\n",
    "    \n",
    "    drop_out2=Dropout(0.5)(flatten)\n",
    "    dense_layer1=Dense(16, activation='relu')(drop_out2)\n",
    "    output_layer=Dense(10, activation='softmax')(dense_layer1)\n",
    "\n",
    "    model = Model(inputs=input_layer,outputs=output_layer)\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T02:56:08.250669Z",
     "iopub.status.busy": "2024-06-03T02:56:08.250326Z",
     "iopub.status.idle": "2024-06-03T02:58:48.183672Z",
     "shell.execute_reply": "2024-06-03T02:58:48.182556Z",
     "shell.execute_reply.started": "2024-06-03T02:56:08.250641Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 15:31:12.169900: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1635/1635 [==============================] - 1239s 751ms/step - loss: 2.2633 - accuracy: 0.1566 - val_loss: 2.2438 - val_accuracy: 0.1874\n",
      "Epoch 2/30\n",
      "1635/1635 [==============================] - 1241s 759ms/step - loss: 2.1433 - accuracy: 0.2185 - val_loss: 2.1033 - val_accuracy: 0.2726\n",
      "Epoch 3/30\n",
      "1635/1635 [==============================] - 1219s 746ms/step - loss: 2.0466 - accuracy: 0.2472 - val_loss: 2.0615 - val_accuracy: 0.2646\n",
      "Epoch 4/30\n",
      "1635/1635 [==============================] - 1182s 723ms/step - loss: 1.9711 - accuracy: 0.2682 - val_loss: 2.0831 - val_accuracy: 0.2514\n",
      "Epoch 5/30\n",
      "1635/1635 [==============================] - 1187s 726ms/step - loss: 1.9056 - accuracy: 0.2898 - val_loss: 2.0460 - val_accuracy: 0.2600\n",
      "Epoch 6/30\n",
      "1635/1635 [==============================] - 1211s 741ms/step - loss: 1.8560 - accuracy: 0.3047 - val_loss: 2.1149 - val_accuracy: 0.2244\n",
      "Epoch 7/30\n",
      "1635/1635 [==============================] - 1233s 754ms/step - loss: 1.8116 - accuracy: 0.3196 - val_loss: 2.0935 - val_accuracy: 0.2380\n",
      "Epoch 8/30\n",
      "1635/1635 [==============================] - 1221s 747ms/step - loss: 1.7702 - accuracy: 0.3359 - val_loss: 2.1095 - val_accuracy: 0.2391\n",
      "Epoch 9/30\n",
      "1635/1635 [==============================] - 1237s 756ms/step - loss: 1.7446 - accuracy: 0.3424 - val_loss: 2.1401 - val_accuracy: 0.2272\n",
      "168/168 [==============================] - 7s 36ms/step\n",
      "168/168 [==============================] - 6s 36ms/step - loss: 2.0460 - accuracy: 0.2600\n",
      "F1 Score do fold 1: 0.24218515997684234\n",
      "Fold 2/5\n",
      "Epoch 1/30\n",
      "1635/1635 [==============================] - 1218s 741ms/step - loss: 2.1544 - accuracy: 0.2057 - val_loss: 2.0358 - val_accuracy: 0.3057\n",
      "Epoch 2/30\n",
      "1635/1635 [==============================] - 1169s 715ms/step - loss: 1.9641 - accuracy: 0.2902 - val_loss: 1.9056 - val_accuracy: 0.3482\n",
      "Epoch 3/30\n",
      "1635/1635 [==============================] - 1181s 722ms/step - loss: 1.8355 - accuracy: 0.3337 - val_loss: 1.8622 - val_accuracy: 0.3509\n",
      "Epoch 4/30\n",
      "1635/1635 [==============================] - 1191s 729ms/step - loss: 1.7304 - accuracy: 0.3665 - val_loss: 1.8791 - val_accuracy: 0.3342\n",
      "Epoch 5/30\n",
      "1635/1635 [==============================] - 1164s 712ms/step - loss: 1.6471 - accuracy: 0.3997 - val_loss: 1.9494 - val_accuracy: 0.3260\n",
      "Epoch 6/30\n",
      "1635/1635 [==============================] - 1166s 713ms/step - loss: 1.5788 - accuracy: 0.4268 - val_loss: 1.9304 - val_accuracy: 0.3240\n",
      "Epoch 7/30\n",
      "1635/1635 [==============================] - 1180s 721ms/step - loss: 1.5160 - accuracy: 0.4452 - val_loss: 2.0297 - val_accuracy: 0.3303\n",
      "168/168 [==============================] - 6s 30ms/step\n",
      "168/168 [==============================] - 5s 32ms/step - loss: 1.8622 - accuracy: 0.3509\n",
      "F1 Score do fold 2: 0.31414039694560836\n",
      "Fold 3/5\n",
      "Epoch 1/30\n",
      "1635/1635 [==============================] - 1215s 738ms/step - loss: 2.1659 - accuracy: 0.1940 - val_loss: 2.0833 - val_accuracy: 0.2499\n",
      "Epoch 2/30\n",
      "1635/1635 [==============================] - 1180s 722ms/step - loss: 2.0517 - accuracy: 0.2524 - val_loss: 2.0760 - val_accuracy: 0.2730\n",
      "Epoch 3/30\n",
      "1635/1635 [==============================] - 843s 516ms/step - loss: 1.9547 - accuracy: 0.2894 - val_loss: 2.0058 - val_accuracy: 0.2970\n",
      "Epoch 4/30\n",
      "1635/1635 [==============================] - 852s 521ms/step - loss: 1.8628 - accuracy: 0.3262 - val_loss: 1.9778 - val_accuracy: 0.3072\n",
      "Epoch 5/30\n",
      "1635/1635 [==============================] - 855s 523ms/step - loss: 1.7861 - accuracy: 0.3522 - val_loss: 1.9975 - val_accuracy: 0.3022\n",
      "Epoch 6/30\n",
      "1635/1635 [==============================] - 845s 517ms/step - loss: 1.7213 - accuracy: 0.3742 - val_loss: 2.0321 - val_accuracy: 0.2977\n",
      "Epoch 7/30\n",
      "1635/1635 [==============================] - 847s 518ms/step - loss: 1.6728 - accuracy: 0.3912 - val_loss: 2.0774 - val_accuracy: 0.2938\n",
      "Epoch 8/30\n",
      "1635/1635 [==============================] - 797s 487ms/step - loss: 1.6336 - accuracy: 0.4057 - val_loss: 2.0587 - val_accuracy: 0.3022\n",
      "168/168 [==============================] - 3s 17ms/step\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 1.9778 - accuracy: 0.3072\n",
      "F1 Score do fold 3: 0.27106912246384646\n",
      "Fold 4/5\n",
      "Epoch 1/30\n",
      "1635/1635 [==============================] - 824s 502ms/step - loss: 2.1798 - accuracy: 0.1967 - val_loss: 2.0597 - val_accuracy: 0.2702\n",
      "Epoch 2/30\n",
      " 476/1635 [=======>......................] - ETA: 9:48 - loss: 2.0547 - accuracy: 0.2570"
     ]
    }
   ],
   "source": [
    "num_folds = 5\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "f1_per_fold = []\n",
    "loss_per_fold = []\n",
    "conf_matrix_list = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X, np.argmax(y, axis=1))):\n",
    "    \n",
    "    print(f'Fold {fold_idx + 1}/{num_folds}')\n",
    "    \n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    model = create_model()\n",
    "    early_stopping = EarlyStopping(patience=4, restore_best_weights=True)\n",
    "    history = model.fit(X_resampled, y_resampled, batch_size=16, epochs=30, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(np.argmax(y_val.astype(int), axis=1),y_pred_classes)\n",
    "    conf_matrix_list.append(conf_matrix)\n",
    "    \n",
    "    f1 = f1_score(np.argmax(y_val, axis=1), y_pred_classes, average='weighted')\n",
    "    f1_per_fold.append(f1)\n",
    "    \n",
    "    scores = model.evaluate(X_val, y_val)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'F1 Score do fold {fold_idx + 1}: {f1}')\n",
    "\n",
    "print(f'\\nF1 Score médio nos {num_folds} folds: {np.mean(f1_per_fold)} (+/- {np.std(f1_per_fold)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T02:58:48.186397Z",
     "iopub.status.busy": "2024-06-03T02:58:48.185875Z",
     "iopub.status.idle": "2024-06-03T02:58:48.948576Z",
     "shell.execute_reply": "2024-06-03T02:58:48.947435Z",
     "shell.execute_reply.started": "2024-06-03T02:58:48.186363Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs_range = range(1, len(history.epoch) + 1)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Train Set')\n",
    "plt.plot(epochs_range, val_acc, label='Val Set')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Train Set')\n",
    "plt.plot(epochs_range, val_loss, label='Val Set')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Criação da matriz de confusão\n",
    "\n",
    "conf_matrix = np.sum(conf_matrix_list, axis=0)\n",
    "\n",
    "# Plotagem da matriz de confusão com Seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(balanced_df['genre']), yticklabels=np.unique(balanced_df['genre']))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 165566,
     "sourceId": 377107,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30715,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "studies",
   "language": "python",
   "name": "studies"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
