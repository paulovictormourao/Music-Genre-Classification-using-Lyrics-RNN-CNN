{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T02:55:47.665408Z",
     "iopub.status.busy": "2024-06-03T02:55:47.664862Z",
     "iopub.status.idle": "2024-06-03T02:56:05.125017Z",
     "shell.execute_reply": "2024-06-03T02:56:05.123918Z",
     "shell.execute_reply.started": "2024-06-03T02:55:47.665358Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 14:14:48.591324: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict,train_test_split, StratifiedKFold \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Concatenate, concatenate, Embedding\n",
    "from tensorflow.keras.layers import MaxPooling1D, Conv1D, GlobalMaxPooling1D, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import he_normal, glorot_normal\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Nadam, Adagrad, Adamax\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T02:56:05.127651Z",
     "iopub.status.busy": "2024-06-03T02:56:05.126996Z",
     "iopub.status.idle": "2024-06-03T02:56:05.133502Z",
     "shell.execute_reply": "2024-06-03T02:56:05.132328Z",
     "shell.execute_reply.started": "2024-06-03T02:56:05.127618Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/paulo/anaconda3/envs/studies/TMCI_Project-master/lyrics.csv' \n",
    "df_full = pd.read_csv(DATA_PATH)\n",
    "df_full = df_full.dropna()\n",
    "df_full = df_full.drop(df_full.loc[(df_full['year'] < 1900) | (df_full['year'] > 2024)].index)\n",
    "df_full = df_full.drop(df_full[(df_full['genre'] == 'Not Available') | (df_full['genre'] == 'Other')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full[['lyrics', 'genre']]\n",
    "\n",
    "def clean_lyrics(text):\n",
    "    text = re.sub(r'[^A-Za-z ]', ' ', text) # Remover caracteres especiais\n",
    "    \n",
    "    words = text.split()  # Remover letras com menos de 3 palavras\n",
    "    if len(words) < 3:\n",
    "        return \"\"\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text.strip()  \n",
    " \n",
    "    \n",
    "df_full['clean_lyrics'] = df_full['lyrics'].apply(clean_lyrics)\n",
    "\n",
    "df_clean = df_full[df_full['clean_lyrics'] != \"\"] # Excluir linhas com menos de 3 palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T02:56:05.160592Z",
     "iopub.status.busy": "2024-06-03T02:56:05.160101Z",
     "iopub.status.idle": "2024-06-03T02:56:07.740956Z",
     "shell.execute_reply": "2024-06-03T02:56:07.739521Z",
     "shell.execute_reply.started": "2024-06-03T02:56:05.160550Z"
    }
   },
   "outputs": [],
   "source": [
    "genre_frequency = df_clean['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(df, target_column, max_instances):\n",
    "    balanced_df = pd.DataFrame() # Ordena o grupo pelo nÃºmero de palavras e seleciona os primeiros max_instances\n",
    "    for class_label, group in df.groupby(target_column):\n",
    "        if len(group) > max_instances:\n",
    "            group = group.assign(word_count=group[target_column].apply(lambda x: len(str(x).split())))\n",
    "            group = group.sort_values(by='word_count', ascending=False).head(max_instances)\n",
    "        balanced_df = pd.concat([balanced_df, group])\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_instances_per_class = 3390\n",
    "balanced_df = balance_classes(df_clean, 'genre',max_instances_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tabs_new_line(lyrics):\n",
    "    string = re.sub(r\"\\s+\",\" \",lyrics)\n",
    "    string = re.sub(r\"[/]\",\" \",string)\n",
    "    return string\n",
    "\n",
    "def decontractions(lyrics):\n",
    "    decontracted = re.sub(r\"won\\'t\", \"will not\", lyrics)\n",
    "    decontracted = re.sub(r\"can\\'t\", \"can not\", decontracted)\n",
    "    decontracted = re.sub(r\"n\\'t\", \" not\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'re\", \" are\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'s\", \" is\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'d\", \" would\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'ll\", \" will\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'t\", \" not\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'ve\", \" have\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'m\", \" am\", decontracted)\n",
    "    decontracted = re.sub(r\"ain\\'t\", \"is not\", decontracted)\n",
    "    decontracted = re.sub(r\"\\'cause\", \"because\", decontracted)\n",
    "    decontracted = re.sub(r\"y\\'all\", \"you all\", decontracted)\n",
    "    decontracted = re.sub(r\"ma\\'am\", \"madam\", decontracted)\n",
    "    decontracted = re.sub(r\"o\\'clock\", \"of the clock\", decontracted)\n",
    "    decontracted = re.sub(r\"gonna\", \"going to\", decontracted)\n",
    "    decontracted = re.sub(r\"wanna\", \"want to\", decontracted)\n",
    "    decontracted = re.sub(r\"gotta\", \"got to\", decontracted)\n",
    "    decontracted = re.sub(r\"let\\'s\", \"let us\", decontracted)\n",
    "    decontracted = re.sub(r\"how\\'d\", \"how did\", decontracted)\n",
    "    decontracted = re.sub(r\"how\\'ll\", \"how will\", decontracted)\n",
    "    decontracted = re.sub(r\"how\\'s\", \"how is\", decontracted)\n",
    "    decontracted = re.sub(r\"what\\'d\", \"what did\", decontracted)\n",
    "    decontracted = re.sub(r\"what\\'ll\", \"what will\", decontracted)\n",
    "    decontracted = re.sub(r\"what\\'re\", \"what are\", decontracted)\n",
    "    decontracted = re.sub(r\"what\\'s\", \"what is\", decontracted)\n",
    "    decontracted = re.sub(r\"what\\'ve\", \"what have\", decontracted)\n",
    "    decontracted = re.sub(r\"where\\'d\", \"where did\", decontracted)\n",
    "    decontracted = re.sub(r\"where\\'ll\", \"where will\", decontracted)\n",
    "    decontracted = re.sub(r\"where\\'re\", \"where are\", decontracted)\n",
    "    decontracted = re.sub(r\"where\\'s\", \"where is\", decontracted)\n",
    "    decontracted = re.sub(r\"where\\'ve\", \"where have\", decontracted)\n",
    "    decontracted = re.sub(r\"who\\'d\", \"who did\", decontracted)\n",
    "    decontracted = re.sub(r\"who\\'ll\", \"who will\", decontracted)\n",
    "    decontracted = re.sub(r\"who\\'re\", \"who are\", decontracted)\n",
    "    decontracted = re.sub(r\"who\\'s\", \"who is\", decontracted)\n",
    "    decontracted = re.sub(r\"who\\'ve\", \"who have\", decontracted)\n",
    "    decontracted = re.sub(r\"why\\'d\", \"why did\", decontracted)\n",
    "    decontracted = re.sub(r\"why\\'ll\", \"why will\", decontracted)\n",
    "    decontracted = re.sub(r\"why\\'re\", \"why are\", decontracted)\n",
    "    decontracted = re.sub(r\"why\\'s\", \"why is\", decontracted)\n",
    "    decontracted = re.sub(r\"why\\'ve\", \"why have\", decontracted)\n",
    "    decontracted = re.sub(r\"that\\'d\", \"that would\", decontracted)\n",
    "    decontracted = re.sub(r\"that\\'ll\", \"that will\", decontracted)\n",
    "    decontracted = re.sub(r\"that\\'re\", \"that are\", decontracted)\n",
    "    decontracted = re.sub(r\"that\\'s\", \"that is\", decontracted)\n",
    "    decontracted = re.sub(r\"that\\'ve\", \"that have\", decontracted)\n",
    "    return decontracted\n",
    "\n",
    "def lower_case(lyrics):\n",
    "    return lyrics.lower() \n",
    "\n",
    "def is_english(lyrics):\n",
    "    try:\n",
    "        detect(lyrics) == 'en'\n",
    "        return lyrics\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "def remove_stopwords(tokens):  \n",
    "    tokens_without_stopwords = [word for word in tokens if word not in STOP_WORDS]\n",
    "    return tokens_without_stopwords\n",
    "\n",
    "def remove_short_words(tokens, N):\n",
    "    filtered_lyrics = [word for word in tokens if len(word) > N]  # Filtra palavras com tamanho >= N\n",
    "    return filtered_lyrics\n",
    "\n",
    "def remove_repeated_words(tokens):\n",
    "    unique_words = []\n",
    "    seen_words = set()\n",
    "    for word in tokens:\n",
    "        if word not in seen_words:\n",
    "            unique_words.append(word)\n",
    "            seen_words.add(word)\n",
    "    return unique_words\n",
    "\n",
    "def final_processing(lyrics):  \n",
    "    string = re.sub(r'[^a-z\\s]', '', lyrics)\n",
    "    string = re.sub(r'\\bverse\\b', '', string)\n",
    "    string = re.sub(r'\\bchorus\\b', '', string)\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(lyrics):\n",
    "    lyrics = remove_tabs_new_line(lyrics)\n",
    "    lyrics = lower_case(lyrics)\n",
    "    #lyrics = is_english(lyrics)\n",
    "    lyrics = decontractions(lyrics) # Decontractions like: I've -> I have\n",
    "    tokens = word_tokenize(lyrics) \n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = remove_short_words(tokens, N=2)\n",
    "    #tokens = remove_repeated_words(tokens)\n",
    "    lyrics = ' '.join(tokens)\n",
    "    lyrics = final_processing(lyrics)\n",
    "    \n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df['processed_lyrics'] = balanced_df['lyrics'].apply(preprocess_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_frequency_treshhold(word_count, N):\n",
    "    N_most_common_words = []\n",
    "    for word, count in word_count.items():\n",
    "        if count > N:\n",
    "            N_most_common_words.append(word)\n",
    "    return N_most_common_words\n",
    "\n",
    "def remove_words(text, words_to_remove):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word not in words_to_remove]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frequency = 1900\n",
    "embeddings_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics = ' '.join(balanced_df['processed_lyrics'])\n",
    "words = all_lyrics.split()\n",
    "\n",
    "word_count = Counter(words)\n",
    "\n",
    "most_common_words = words_frequency_treshhold(word_count, max_frequency)\n",
    "\n",
    "\n",
    "balanced_df['processed_lyrics'] = balanced_df['processed_lyrics'].apply(lambda lyrics: remove_words(lyrics, most_common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "\n",
    "with open('unbalanced_vectors'+str(max_frequency)+'.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=np.float32)\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(balanced_df['processed_lyrics'])\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, embeddings_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = tokenizer.texts_to_sequences(balanced_df['processed_lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32316,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df['encoded_docs'] = encoded_docs\n",
    "balanced_df['encoded_docs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "Hip-Hop       3269\n",
       "Metal         2988\n",
       "Pop           2957\n",
       "Country       2761\n",
       "Rock          2753\n",
       "R&B           2651\n",
       "Jazz          2548\n",
       "Indie         2537\n",
       "Electronic    2525\n",
       "Folk          1880\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtred = balanced_df[balanced_df['encoded_docs'].apply(len) >= 20]\n",
    "df_filtred['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 170\n",
    "X = pad_sequences(df_filtred['encoded_docs'], maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29600, 170)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(df_filtred['genre'])\n",
    "y = one_hot.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_layer =  Input(shape=(max_length),dtype='int32')\n",
    "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=True)(input_layer)\n",
    "    \n",
    "    x1=Conv1D(3,7,kernel_initializer='glorot_uniform',kernel_regularizer=l2(),activation='relu')(e)\n",
    "    x2=Conv1D(3,8,kernel_initializer='glorot_uniform',kernel_regularizer=l2(),activation='relu')(e)\n",
    "    x3=Conv1D(3,6,kernel_initializer='glorot_uniform',kernel_regularizer=l2(),activation='relu')(e)\n",
    "\n",
    "    concatted1= concatenate([x1,x2,x3],axis=1)\n",
    "    \n",
    "    max_pool1=MaxPooling1D(3)(concatted1)\n",
    "\n",
    "    y1=Conv1D(3,9,kernel_initializer='glorot_uniform',activation='relu')(max_pool1)\n",
    "    y2=Conv1D(3,4,kernel_initializer='glorot_uniform',activation='relu')(max_pool1)\n",
    "    y3=Conv1D(3,5,kernel_initializer='glorot_uniform',activation='relu')(max_pool1)\n",
    "\n",
    "    concatted2 =concatenate([y1,y2,y3],axis=1)\n",
    "    max_pool2=MaxPooling1D(3)(concatted2)\n",
    "    \n",
    "    drop_out=Dropout(0.5)(max_pool1)\n",
    "    max_pool3=MaxPooling1D(3)(drop_out)\n",
    "\n",
    "    conv_layer1=Conv1D(3,12, activation='relu')(max_pool3)\n",
    "\n",
    "    flatten=Flatten()(conv_layer1)\n",
    "    \n",
    "    drop_out2=Dropout(0.5)(flatten)\n",
    "    dense_layer1=Dense(16, activation='relu')(drop_out2)\n",
    "    output_layer=Dense(10, activation='softmax')(dense_layer1)\n",
    "\n",
    "    model = Model(inputs=input_layer,outputs=output_layer)\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T02:56:08.250669Z",
     "iopub.status.busy": "2024-06-03T02:56:08.250326Z",
     "iopub.status.idle": "2024-06-03T02:58:48.183672Z",
     "shell.execute_reply": "2024-06-03T02:58:48.182556Z",
     "shell.execute_reply.started": "2024-06-03T02:56:08.250641Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 14:32:09.155194: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "828/828 [==============================] - 336s 402ms/step - loss: 2.1770 - accuracy: 0.2007 - val_loss: 2.0402 - val_accuracy: 0.2883\n",
      "Epoch 2/30\n",
      "828/828 [==============================] - 341s 412ms/step - loss: 1.9724 - accuracy: 0.2842 - val_loss: 1.9244 - val_accuracy: 0.3090\n",
      "Epoch 3/30\n",
      "828/828 [==============================] - 334s 403ms/step - loss: 1.8505 - accuracy: 0.3308 - val_loss: 1.9001 - val_accuracy: 0.3262\n",
      "Epoch 4/30\n",
      "828/828 [==============================] - 335s 405ms/step - loss: 1.7391 - accuracy: 0.3745 - val_loss: 1.8936 - val_accuracy: 0.3394\n",
      "Epoch 5/30\n",
      "828/828 [==============================] - 335s 404ms/step - loss: 1.6472 - accuracy: 0.4148 - val_loss: 1.9180 - val_accuracy: 0.3343\n",
      "Epoch 6/30\n",
      "545/828 [==================>...........] - ETA: 1:55 - loss: 1.5469 - accuracy: 0.4493"
     ]
    }
   ],
   "source": [
    "num_folds = 5\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "f1_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X, np.argmax(y, axis=1))):\n",
    "    \n",
    "    print(f'Fold {fold_idx + 1}/{num_folds}')\n",
    "    \n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    model = create_model()\n",
    "    early_stopping = EarlyStopping(patience=4, restore_best_weights=True)\n",
    "    history = model.fit(X_resampled,y_resampled, batch_size=32, epochs=30, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "    y_pred = model.predict(X_val)  # Validation Test\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    f1 = f1_score(np.argmax(y_val, axis=1), y_pred_classes, average='weighted') # Metrics\n",
    "    f1_per_fold.append(f1)\n",
    "    scores = model.evaluate(X_val, y_val) \n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    print(f'F1 Score do fold {fold_idx + 1}: {f1}')\n",
    "\n",
    "print(f'\\nF1 Score mÃ©dio nos {num_folds} folds: {np.mean(f1_per_fold)} (+/- {np.std(f1_per_fold)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T02:58:48.186397Z",
     "iopub.status.busy": "2024-06-03T02:58:48.185875Z",
     "iopub.status.idle": "2024-06-03T02:58:48.948576Z",
     "shell.execute_reply": "2024-06-03T02:58:48.947435Z",
     "shell.execute_reply.started": "2024-06-03T02:58:48.186363Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs_range = range(1, len(history.epoch) + 1)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Train Set')\n",
    "plt.plot(epochs_range, val_acc, label='Val Set')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Train Set')\n",
    "plt.plot(epochs_range, val_loss, label='Val Set')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# CriaÃ§Ã£o da matriz de confusÃ£o\n",
    "y_val_classes = np.argmax(y_val.astype(int), axis=1)\n",
    "conf_matrix = confusion_matrix(y_val_classes, y_pred_classes)\n",
    "\n",
    "# Plotagem da matriz de confusÃ£o com Seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(balanced_df['genre']), yticklabels=np.unique(balanced_df['genre']))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 165566,
     "sourceId": 377107,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30715,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "studies",
   "language": "python",
   "name": "studies"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
